import asyncio
import edge_tts
import os

class TTSEngine:
    default_voice = "en-US-ChristopherNeural"

    @staticmethod
    async def generate_audio(text: str, output_path: str, voice: str = None):
        """
        Generates high-quality English narration and word-level timestamps.
        Returns: (audio_path, word_offsets)
        """
        if not voice:
            # Switch to GuyNeural which often has more robust metadata in cloud envs
            voice = "en-US-GuyNeural"
            
        communicate = edge_tts.Communicate(text, voice)
        word_offsets = []
        chunk_types = set()
        
        try:
            audio_data = bytearray()
            async for chunk in communicate.stream():
                ctype = chunk.get("type") or chunk.get("Type") or "unknown"
                chunk_types.add(ctype)
                
                if ctype == "audio":
                    audio_data.extend(chunk["data"])
                elif ctype == "WordBoundary":
                    word_offsets.append({
                        "word": chunk.get("text") or chunk.get("Text"),
                        "start": (chunk.get("offset") or chunk.get("Offset")) / 10**7,
                        "duration": (chunk.get("duration") or chunk.get("Duration")) / 10**7
                    })

            with open(output_path, "wb") as f:
                f.write(audio_data)

        except Exception as e:
            print(f"Error during TTS streaming: {e}")
        
        # --- FALLBACK: Simulated Word Offsets ---
        # If WordBoundary events are missing, we manually estimate them based on duration.
        if not word_offsets and len(text.strip()) > 0:
            print("FALLBACK: Simulated Word Sync initiated.")
            from moviepy.editor import AudioFileClip
            try:
                temp_audio = AudioFileClip(output_path)
                total_dur = temp_audio.duration
                words = text.split()
                # Average duration per word
                avg_word_dur = total_dur / len(words)
                
                start_time = 0
                for w in words:
                    # Logic: use word length to adjust slightly for better realism
                    w_dur = (len(w) / sum(len(x) for x in words)) * total_dur
                    word_offsets.append({
                        "word": w,
                        "start": start_time,
                        "duration": w_dur
                    })
                    start_time += w_dur
            except Exception as e:
                print(f"Simulated sync failed: {e}")

        if not word_offsets:
            print(f"CRITICAL: Failed to generate word offsets for synchronization.")
        else:
            print(f"SUCCESS: {len(word_offsets)} word milestones ready for animation.")
        
        return output_path, word_offsets

if __name__ == "__main__":
    # Test generation
    text = "Breaking news: A major international event is unfolding right now. More updates will follow."
    output = "test_narration.mp3"
    asyncio.run(TTSEngine.generate_audio(text, output))
